{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL and DataFrames \n",
    "\n",
    "<a href = \"http://yogen.io\"><img src=\"http://yogen.io/assets/logo.svg\" alt=\"yogen\" style=\"width: 200px; float: right;\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs, DataSets, and DataFrames\n",
    "\n",
    "RDDs are the original interface for Spark programming.\n",
    "\n",
    "DataFrames were introduced in 1.3\n",
    "\n",
    "Datasets were introduced in 1.6, and unified with DataFrames in 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of DataFrames:\n",
    "\n",
    "from https://www.datacamp.com/community/tutorials/apache-spark-python:\n",
    "\n",
    "> More specifically, the performance improvements are due to two things, which you’ll often come across when you’re reading up DataFrames: custom memory management (project Tungsten), which will make sure that your Spark jobs much faster given CPU constraints, and optimized execution plans (Catalyst optimizer), of which the logical plan of the DataFrame is a part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSQL and DataFrames \n",
    "\n",
    "\n",
    "pyspark does not have the Dataset API, which is available only if you use Spark from a statically typed language: Scala or Java.\n",
    "\n",
    "From https://spark.apache.org/docs/2.2.0/sql-programming-guide.html:\n",
    "\n",
    "> A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrame is simply a type alias of Dataset[Row]. While, in Java API, users need to use Dataset&lt;Row> to represent a DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pyspark.sql module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important classes of Spark SQL and DataFrames:\n",
    "\n",
    "* `pyspark.sql.SparkSession` Main entry point for DataFrame and SQL functionality.\n",
    "\n",
    "* `pyspark.sql.DataFrame` A distributed collection of data grouped into named columns.\n",
    "\n",
    "* `pyspark.sql.Column` A column expression in a DataFrame.\n",
    "\n",
    "* `pyspark.sql.Row` A row of data in a DataFrame.\n",
    "\n",
    "* `pyspark.sql.GroupedData` Aggregation methods, returned by DataFrame.groupBy().\n",
    "\n",
    "* `pyspark.sql.DataFrameNaFunctions` Methods for handling missing data (null values).\n",
    "\n",
    "* `pyspark.sql.DataFrameStatFunctions` Methods for statistics functionality.\n",
    "\n",
    "* `pyspark.sql.functions` List of built-in functions available for DataFrame.\n",
    "\n",
    "* `pyspark.sql.types` List of data types available.\n",
    "\n",
    "* `pyspark.sql.Window` For working with window functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession\n",
    "\n",
    "The traditional way to interact with Spark is the SparkContext. In the notebooks we get that from the pyspark driver.\n",
    "\n",
    "From 2.0 we can use SparkSession to replace SparkConf, SparkContext and SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "session = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1536995346147'),\n",
       " ('config.option.key', 'value'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', '10.0.2.15'),\n",
       " ('spark.driver.port', '42069'),\n",
       " ('spark.app.name', 'PySparkShell')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Así es como se meten opciones y se cambian\n",
    "session = SparkSession.builder.config(\"config.option.key\", \"value\").getOrCreate()\n",
    "\n",
    "session.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passing other options to spark session:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check option values in the resulting session like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames\n",
    "\n",
    "SparkSession.createDataFrame: from an RDD, a list or a pandas.DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.createDataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames\n",
    "\n",
    "* From lists\n",
    "* From RDDs\n",
    "* from Hive tables\n",
    "* From Spark sources: parquet (default), json, jdbc, orc, libsvm, csv, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project manager',\n",
       " 'teacher',\n",
       " 'teacher',\n",
       " 'project manager',\n",
       " 'data scientist',\n",
       " 'teacher',\n",
       " 'teacher',\n",
       " 'teacher',\n",
       " 'project manager',\n",
       " 'teacher',\n",
       " 'project manager',\n",
       " 'project manager']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "ids = range(12)\n",
    "positions = [random.choice([\"teacher\", \"data scientist\", \"project manager\"]) for _ in ids]\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=0, _2='project manager'),\n",
       " Row(_1=1, _2='teacher'),\n",
       " Row(_1=2, _2='teacher'),\n",
       " Row(_1=3, _2='project manager'),\n",
       " Row(_1=4, _2='data scientist'),\n",
       " Row(_1=5, _2='teacher'),\n",
       " Row(_1=6, _2='teacher'),\n",
       " Row(_1=7, _2='teacher'),\n",
       " Row(_1=8, _2='project manager'),\n",
       " Row(_1=9, _2='teacher'),\n",
       " Row(_1=10, _2='project manager'),\n",
       " Row(_1=11, _2='project manager')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = session.createDataFrame(zip(ids, positions))\n",
    "\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=0, _2='project manager'),\n",
       " Row(_1=1, _2='teacher'),\n",
       " Row(_1=2, _2='teacher'),\n",
       " Row(_1=3, _2='project manager'),\n",
       " Row(_1=4, _2='data scientist')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| _1|             _2|\n",
      "+---+---------------+\n",
      "|  0|project manager|\n",
      "|  1|        teacher|\n",
      "|  2|        teacher|\n",
      "|  3|project manager|\n",
      "|  4| data scientist|\n",
      "+---+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Row in module pyspark.sql.types:\n",
      "\n",
      "class Row(builtins.tuple)\n",
      " |  A row in L{DataFrame}.\n",
      " |  The fields in it can be accessed:\n",
      " |  \n",
      " |  * like attributes (``row.key``)\n",
      " |  * like dictionary values (``row[key]``)\n",
      " |  \n",
      " |  ``key in row`` will search through row keys.\n",
      " |  \n",
      " |  Row can be used to create a row object by using named arguments,\n",
      " |  the fields will be sorted by names. It is not allowed to omit\n",
      " |  a named argument to represent the value is None or missing. This should be\n",
      " |  explicitly set to None in this case.\n",
      " |  \n",
      " |  >>> row = Row(name=\"Alice\", age=11)\n",
      " |  >>> row\n",
      " |  Row(age=11, name='Alice')\n",
      " |  >>> row['name'], row['age']\n",
      " |  ('Alice', 11)\n",
      " |  >>> row.name, row.age\n",
      " |  ('Alice', 11)\n",
      " |  >>> 'name' in row\n",
      " |  True\n",
      " |  >>> 'wrong_key' in row\n",
      " |  False\n",
      " |  \n",
      " |  Row also can be used to create another Row like class, then it\n",
      " |  could be used to create Row objects, such as\n",
      " |  \n",
      " |  >>> Person = Row(\"name\", \"age\")\n",
      " |  >>> Person\n",
      " |  <Row(name, age)>\n",
      " |  >>> 'name' in Person\n",
      " |  True\n",
      " |  >>> 'wrong_key' in Person\n",
      " |  False\n",
      " |  >>> Person(\"Alice\", 11)\n",
      " |  Row(name='Alice', age=11)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Row\n",
      " |      builtins.tuple\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, *args)\n",
      " |      create new Row object\n",
      " |  \n",
      " |  __contains__(self, item)\n",
      " |      Return key in self.\n",
      " |  \n",
      " |  __getattr__(self, item)\n",
      " |  \n",
      " |  __getitem__(self, item)\n",
      " |      Return self[key].\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      Returns a tuple so Python knows how to pickle Row.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Printable representation of Row used in Python REPL.\n",
      " |  \n",
      " |  __setattr__(self, key, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  asDict(self, recursive=False)\n",
      " |      Return as an dict\n",
      " |      \n",
      " |      :param recursive: turns the nested Row as dict (default: False).\n",
      " |      \n",
      " |      >>> Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}\n",
      " |      True\n",
      " |      >>> row = Row(key=1, value=Row(name='a', age=2))\n",
      " |      >>> row.asDict() == {'key': 1, 'value': Row(age=2, name='a')}\n",
      " |      True\n",
      " |      >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n",
      " |      True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(self, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.tuple:\n",
      " |  \n",
      " |  __add__(self, value, /)\n",
      " |      Return self+value.\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getnewargs__(...)\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self, /)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __mul__(self, value, /)\n",
      " |      Return self*value.n\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __rmul__(self, value, /)\n",
      " |      Return self*value.\n",
      " |  \n",
      " |  count(...)\n",
      " |      T.count(value) -> integer -- return number of occurrences of value\n",
      " |  \n",
      " |  index(...)\n",
      " |      T.index(value, [start, [stop]]) -> integer -- return first index of value.\n",
      " |      Raises ValueError if the value is not present.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "help(Row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es para definir el esquema, es poner el nombre y tipo de las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, position='project manager'),\n",
       " Row(id=1, position='teacher'),\n",
       " Row(id=2, position='teacher'),\n",
       " Row(id=3, position='project manager'),\n",
       " Row(id=4, position='data scientist'),\n",
       " Row(id=5, position='teacher'),\n",
       " Row(id=6, position='teacher'),\n",
       " Row(id=7, position='teacher'),\n",
       " Row(id=8, position='project manager'),\n",
       " Row(id=9, position='teacher'),\n",
       " Row(id=10, position='project manager'),\n",
       " Row(id=11, position='project manager')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = [Row(id=id, position=position) for id, position in zip(ids, positions)]\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, position: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = session.createDataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[employee_id: bigint, employee_position: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.createDataFrame(zip(ids, positions), schema = [\"employee_id\", \"employee_position\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coupon150720.csv MapPartitionsRDD[22] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = session.sparkContext.textFile(\"coupon150720.csv\")\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['79062005698500,1,MAA,AUH,9W,9W,56.79,USD,1,H,H,0526,150904,OK,IAF0',\n",
       " '79062005698500,2,AUH,CDG,9W,9W,84.34,USD,1,H,H,6120,150905,OK,IAF0',\n",
       " '79062005924069,1,CJB,MAA,9W,9W,60.0,USD,1,H,H,2768,150721,OK,IAA0',\n",
       " '79065668570385,1,DEL,DXB,9W,9W,160.63,USD,2,S,S,0546,150804,OK,INA0',\n",
       " '79065668737021,1,AUH,IXE,9W,9W,152.46,USD,1,V,V,0501,150803,OK,INA0']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que cada cupón es una string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para separarlo vamos a hacer split. Y no hacemos flatmap, hacemos map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['79062005698500',\n",
       "  '1',\n",
       "  'MAA',\n",
       "  'AUH',\n",
       "  '9W',\n",
       "  '9W',\n",
       "  '56.79',\n",
       "  'USD',\n",
       "  '1',\n",
       "  'H',\n",
       "  'H',\n",
       "  '0526',\n",
       "  '150904',\n",
       "  'OK',\n",
       "  'IAF0'],\n",
       " ['79062005698500',\n",
       "  '2',\n",
       "  'AUH',\n",
       "  'CDG',\n",
       "  '9W',\n",
       "  '9W',\n",
       "  '84.34',\n",
       "  'USD',\n",
       "  '1',\n",
       "  'H',\n",
       "  'H',\n",
       "  '6120',\n",
       "  '150905',\n",
       "  'OK',\n",
       "  'IAF0'],\n",
       " ['79062005924069',\n",
       "  '1',\n",
       "  'CJB',\n",
       "  'MAA',\n",
       "  '9W',\n",
       "  '9W',\n",
       "  '60.0',\n",
       "  'USD',\n",
       "  '1',\n",
       "  'H',\n",
       "  'H',\n",
       "  '2768',\n",
       "  '150721',\n",
       "  'OK',\n",
       "  'IAA0'],\n",
       " ['79065668570385',\n",
       "  '1',\n",
       "  'DEL',\n",
       "  'DXB',\n",
       "  '9W',\n",
       "  '9W',\n",
       "  '160.63',\n",
       "  'USD',\n",
       "  '2',\n",
       "  'S',\n",
       "  'S',\n",
       "  '0546',\n",
       "  '150804',\n",
       "  'OK',\n",
       "  'INA0'],\n",
       " ['79065668737021',\n",
       "  '1',\n",
       "  'AUH',\n",
       "  'IXE',\n",
       "  '9W',\n",
       "  '9W',\n",
       "  '152.46',\n",
       "  'USD',\n",
       "  '1',\n",
       "  'V',\n",
       "  'V',\n",
       "  '0501',\n",
       "  '150803',\n",
       "  'OK',\n",
       "  'INA0'],\n",
       " ['79062006192650',\n",
       "  '1',\n",
       "  'RPR',\n",
       "  'BOM',\n",
       "  '9W',\n",
       "  '9W',\n",
       "  '68.5',\n",
       "  'USD',\n",
       "  '1',\n",
       "  'K',\n",
       "  'K',\n",
       "  '2202',\n",
       "  '150720',\n",
       "  'OK',\n",
       "  'IAE0'],\n",
       " ['79062006192650',\n",
       "  '2',\n",
       "  'BOM',\n",
       "  'RPR',\n",
       "  '9W',\n",
       "  '9W',\n",
       "  '68.5',\n",
       "  'USD',\n",
       "  '1',\n",
       "  'H',\n",
       "  'H',\n",
       "  '0377',\n",
       "  '150721',\n",
       "  'OK',\n",
       "  'IAE0'],\n",
       " ['79062005733853',\n",
       "  '1',\n",
       "  'DEL',\n",
       "  'DED',\n",
       "  '9W',\n",
       "  '9W',\n",
       "  '56.16',\n",
       "  'USD',\n",
       "  '1',\n",
       "  'V',\n",
       "  'V',\n",
       "  '2839',\n",
       "  '150801',\n",
       "  'OK',\n",
       "  'INA0'],\n",
       " ['79062005836987',\n",
       "  '1',\n",
       "  'ATL',\n",
       "  'LGA',\n",
       "  'AA',\n",
       "  'AA',\n",
       "  '28.3',\n",
       "  'USD',\n",
       "  '1',\n",
       "  'V',\n",
       "  'V',\n",
       "  '3237',\n",
       "  '150903',\n",
       "  'OK',\n",
       "  'INB0'],\n",
       " ['79062005836987',\n",
       "  '2',\n",
       "  'LGA',\n",
       "  'EWR',\n",
       "  '',\n",
       "  '',\n",
       "  '0.0',\n",
       "  'USD',\n",
       "  '1',\n",
       "  '',\n",
       "  '',\n",
       "  'VOID',\n",
       "  '',\n",
       "  '',\n",
       "  'INA0'],\n",
       " ['79062005836987',\n",
       "  '3',\n",
       "  'EWR',\n",
       "  'BOM',\n",
       "  '9W',\n",
       "  '9W',\n",
       "  '176.09',\n",
       "  'USD',\n",
       "  '1',\n",
       "  'W',\n",
       "  'W',\n",
       "  '0227',\n",
       "  '150903',\n",
       "  'OK',\n",
       "  'INB0'],\n",
       " ['79062005836987',\n",
       "  '4',\n",
       "  'BOM',\n",
       "  'AMD',\n",
       "  '9W',\n",
       "  '9W',\n",
       "  '12.02',\n",
       "  'USD',\n",
       "  '1',\n",
       "  'T',\n",
       "  'T',\n",
       "  '2001',\n",
       "  '150905',\n",
       "  'OK',\n",
       "  'INJ0'],\n",
       " ['79062005836987',\n",
       "  '5',\n",
       "  'AMD',\n",
       "  'BOM',\n",
       "  '9W',\n",
       "  '9W',\n",
       "  '12.02',\n",
       "  'USD',\n",
       "  '2',\n",
       "  'T',\n",
       "  'T',\n",
       "  '7104',\n",
       "  '150921',\n",
       "  'OK',\n",
       "  'INJ0'],\n",
       " ['79062005836987',\n",
       "  '6',\n",
       "  'BOM',\n",
       "  'EWR',\n",
       "  '9W',\n",
       "  '9W',\n",
       "  '176.09',\n",
       "  'USD',\n",
       "  '2',\n",
       "  'W',\n",
       "  'W',\n",
       "  '0228',\n",
       "  '150922',\n",
       "  'OK',\n",
       "  'INB0'],\n",
       " ['79062005836987',\n",
       "  '7',\n",
       "  'EWR',\n",
       "  'ATL',\n",
       "  'UA',\n",
       "  'UA',\n",
       "  '28.3',\n",
       "  'USD',\n",
       "  '2',\n",
       "  'K',\n",
       "  'K',\n",
       "  '0568',\n",
       "  '150922',\n",
       "  'OK',\n",
       "  'INB0'],\n",
       " ['79062005862818',\n",
       "  '1',\n",
       "  'LHR',\n",
       "  'BOM',\n",
       "  '9W',\n",
       "  '9W',\n",
       "  '101.65',\n",
       "  'USD',\n",
       "  '1',\n",
       "  'O',\n",
       "  'O',\n",
       "  '0119',\n",
       "  '150820',\n",
       "  'OK',\n",
       "  'INB0'],\n",
       " ['79062005862818',\n",
       "  '2',\n",
       "  'BOM',\n",
       "  'HKG',\n",
       "  '9W',\n",
       "  '9W',\n",
       "  '57.76',\n",
       "  'USD',\n",
       "  '1',\n",
       "  'T',\n",
       "  'T',\n",
       "  '0076',\n",
       "  '150821',\n",
       "  'OK',\n",
       "  'INB0'],\n",
       " ['79065668498895',\n",
       "  '1',\n",
       "  'DXB',\n",
       "  'BOM',\n",
       "  '9W',\n",
       "  '9W',\n",
       "  '54.44',\n",
       "  'USD',\n",
       "  '1',\n",
       "  'V',\n",
       "  'V',\n",
       "  '0543',\n",
       "  '151105',\n",
       "  'OK',\n",
       "  'INAQ'],\n",
       " ['79065668498895',\n",
       "  '2',\n",
       "  'BOM',\n",
       "  'DXB',\n",
       "  '9W',\n",
       "  '9W',\n",
       "  '73.5',\n",
       "  'USD',\n",
       "  '2',\n",
       "  'K',\n",
       "  'K',\n",
       "  '0544',\n",
       "  '151114',\n",
       "  'OK',\n",
       "  'INAQ'],\n",
       " ['79062005534346',\n",
       "  '1',\n",
       "  'KTM',\n",
       "  'DEL',\n",
       "  '9W',\n",
       "  '9W',\n",
       "  '24.92',\n",
       "  'USD',\n",
       "  '1',\n",
       "  'W',\n",
       "  'W',\n",
       "  '0259',\n",
       "  '150722',\n",
       "  'OK',\n",
       "  'INAQ']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "split_rdd.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: string, _3: string, _4: string, _5: string, _6: string, _7: string, _8: string, _9: string, _10: string, _11: string, _12: string, _13: string, _14: string, _15: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.createDataFrame(split_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que no hay nombres ni tipos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring and specifying schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List())"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "types.IntegerType()\n",
    "types.StructType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructField(id,IntegerType,false)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types.StructField(\"id\", types.IntegerType(), nullable=False) # Nullable igual a False quiere decir que es obligatorio que esté relleno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructField(position,StringType,true)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types.StructField(\"position\", types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = types.StructType([types.StructField(\"id\", types.IntegerType(), nullable=False),\n",
    "                  types.StructField(\"position\", types.StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, position: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = session.createDataFrame(zip(ids, positions), schema = schema)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- position: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vemos que el data frame tiene el nombre y el tipo que queremos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully specifying a schema\n",
    "\n",
    "We need to create a `StructType` composed of `StructField`s. each of those specifies afiled with name, type and `nullable` properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From csv files\n",
    "\n",
    "We can either read them directly into dataframes or read them as RDDs and transform that into a DataFrame. This second way will be very useful if we have unstructured data like web server logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+---+---+---+---+-----+---+---+---+----+----+------+----+----+\n",
      "|           _c0|_c1|_c2|_c3|_c4|_c5|  _c6|_c7|_c8|_c9|_c10|_c11|  _c12|_c13|_c14|\n",
      "+--------------+---+---+---+---+---+-----+---+---+---+----+----+------+----+----+\n",
      "|79062005698500|  1|MAA|AUH| 9W| 9W|56.79|USD|  1|  H|   H|0526|150904|  OK|IAF0|\n",
      "|79062005698500|  2|AUH|CDG| 9W| 9W|84.34|USD|  1|  H|   H|6120|150905|  OK|IAF0|\n",
      "+--------------+---+---+---+---+---+-----+---+---+---+----+----+------+----+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_csv = session.read.csv(\"coupon150720.csv\", inferSchema=True)\n",
    "df_from_csv.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: long (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: double (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"SELECT * FROM csv. `coupon150720.csv`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+---+---+---+---+-----+---+---+---+----+----+------+----+----+\n",
      "|           _c0|_c1|_c2|_c3|_c4|_c5|  _c6|_c7|_c8|_c9|_c10|_c11|  _c12|_c13|_c14|\n",
      "+--------------+---+---+---+---+---+-----+---+---+---+----+----+------+----+----+\n",
      "|79062005639642|  1|MAD|BRU| 9W| 9W|15.39|USD|  1|  O|   O|6032|150902|  OK|INB0|\n",
      "|79065668754871|  1|MAD|BRU| SN| 9W|18.56|USD|  1|  W|   W|6032|150921|  OK|INB0|\n",
      "|79062006141366|  1|MAD|CDG| AF| AF|60.92|USD|  1|  K|   K|1401|150806|  OK|INB0|\n",
      "|79062006133090|  1|MAD|CDG| AF| AF| 3.38|USD|  1|  X|   X|1101|151027|  OK|IAE0|\n",
      "|79062006137072|  1|MAD|CDG| AF| AF|26.45|USD|  1|  V|   V|1801|150802|  OK|INA0|\n",
      "+--------------+---+---+---+---+---+-----+---+---+---+----+----+------+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql(\"SELECT * FROM csv.`coupon150720.csv` WHERE _c2='MAD'\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From other types of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Parquet is a free and open-source column-oriented data store of the Apache Hadoop ecosystem. It is similar to the other columnar storage file formats available in Hadoop namely RCFile and Optimized RCFile. It is compatible with most of the data processing frameworks in the Hadoop environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic operations with DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, position: string]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, position='project manager'),\n",
       " Row(id=1, position='teacher'),\n",
       " Row(id=2, position='teacher'),\n",
       " Row(id=3, position='project manager'),\n",
       " Row(id=4, position='data scientist')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| id|       position|\n",
      "+---+---------------+\n",
      "|  0|project manager|\n",
      "|  1|        teacher|\n",
      "|  2|        teacher|\n",
      "|  3|project manager|\n",
      "|  4| data scientist|\n",
      "+---+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- position: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and selecting\n",
    "\n",
    "Syntax inspired in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|       position|\n",
      "+---------------+\n",
      "|project manager|\n",
      "|        teacher|\n",
      "|        teacher|\n",
      "|project manager|\n",
      "| data scientist|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"position\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to filter, we will need to build an instance of `Column`, using square bracket notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| id|       position|\n",
      "+---+---------------+\n",
      "|  6|        teacher|\n",
      "|  7|        teacher|\n",
      "|  8|project manager|\n",
      "|  9|        teacher|\n",
      "| 10|project manager|\n",
      "| 11|project manager|\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"id\"] > 5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`where` is exactly synonimous with `filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| id|       position|\n",
      "+---+---------------+\n",
      "|  6|        teacher|\n",
      "|  7|        teacher|\n",
      "|  8|project manager|\n",
      "|  9|        teacher|\n",
      "| 10|project manager|\n",
      "| 11|project manager|\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df[\"id\"] > 5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'(id > 5)'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = df[\"id\"] > 5\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-03a506e96765>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "col.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's because a comparison between str and int will error out, so spark will not even get the chance to infer to which column we are referring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A column is quite different to a Pandas Series. It is just a reference to a column, and can only be used to construct sparkSQL expressions (select, where...). It can't be collected or taken as a one-dimensional sequence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Extract all employee ids which correspond to data scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"position\"] == \"data scientist\").select(\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding columns\n",
    "\n",
    "Dataframes are immutable, since they are built on top of RDDs, so we can not assign to them. We need to create new DataFrames with the appropriate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------+\n",
      "| id|       position|square|\n",
      "+---+---------------+------+\n",
      "|  0|project manager|   0.0|\n",
      "|  1|        teacher|   1.0|\n",
      "|  2|        teacher|   4.0|\n",
      "|  3|project manager|   9.0|\n",
      "|  4| data scientist|  16.0|\n",
      "|  5|        teacher|  25.0|\n",
      "|  6|        teacher|  36.0|\n",
      "|  7|        teacher|  49.0|\n",
      "|  8|project manager|  64.0|\n",
      "|  9|        teacher|  81.0|\n",
      "| 10|project manager| 100.0|\n",
      "| 11|project manager| 121.0|\n",
      "+---+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.withColumn(\"square\", df[\"id\"] ** 2)\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------+\n",
      "| id|       position|square|\n",
      "+---+---------------+------+\n",
      "|  0|project manager|   0.0|\n",
      "|  1|        teacher|   1.0|\n",
      "|  2|        teacher|   4.0|\n",
      "|  3|project manager|   9.0|\n",
      "|  4| data scientist|  16.0|\n",
      "|  5|        teacher|  25.0|\n",
      "|  6|        teacher|  36.0|\n",
      "|  7|        teacher|  49.0|\n",
      "|  8|project manager|  64.0|\n",
      "|  9|        teacher|  81.0|\n",
      "| 10|project manager| 100.0|\n",
      "| 11|project manager| 121.0|\n",
      "+---+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.select(\"id\",\n",
    "                  \"position\",\n",
    "                  (df[\"id\"] ** 2).alias(\"square\"))\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User defined functions\n",
    "\n",
    "There are many useful functions in pyspark.sql.functions. These work on columns, that is, they are vectorial.\n",
    "\n",
    "We can write User Defined Functions (`udf`s), which allow us to \"vectorize\" operations: write a standard function to process single elements, then build a udf with that that works on columns in a DataFrame, like a SQL function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module pyspark.sql.functions in pyspark.sql:\n",
      "\n",
      "NAME\n",
      "    pyspark.sql.functions - A collections of builtin functions\n",
      "\n",
      "FUNCTIONS\n",
      "    abs(col)\n",
      "        Computes the absolute value.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    acos(col)\n",
      "        Computes the cosine inverse of the given value; the returned angle is in the range0.0 through pi.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    add_months(start, months)\n",
      "        Returns the date that is `months` months after `start`\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['d'])\n",
      "        >>> df.select(add_months(df.d, 1).alias('d')).collect()\n",
      "        [Row(d=datetime.date(2015, 5, 8))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    approxCountDistinct(col, rsd=None)\n",
      "        .. note:: Deprecated in 2.1, use approx_count_distinct instead.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    approx_count_distinct(col, rsd=None)\n",
      "        Returns a new :class:`Column` for approximate distinct count of ``col``.\n",
      "        \n",
      "        >>> df.agg(approx_count_distinct(df.age).alias('c')).collect()\n",
      "        [Row(c=2)]\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    array(*cols)\n",
      "        Creates a new array column.\n",
      "        \n",
      "        :param cols: list of column names (string) or list of :class:`Column` expressions that have\n",
      "            the same data type.\n",
      "        \n",
      "        >>> df.select(array('age', 'age').alias(\"arr\")).collect()\n",
      "        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n",
      "        >>> df.select(array([df.age, df.age]).alias(\"arr\")).collect()\n",
      "        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    array_contains(col, value)\n",
      "        Collection function: returns null if the array is null, true if the array contains the\n",
      "        given value, and false otherwise.\n",
      "        \n",
      "        :param col: name of column containing array\n",
      "        :param value: value to check for in array\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n",
      "        >>> df.select(array_contains(df.data, \"a\")).collect()\n",
      "        [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    asc(col)\n",
      "        Returns a sort expression based on the ascending order of the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    ascii(col)\n",
      "        Computes the numeric value of the first character of the string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    asin(col)\n",
      "        Computes the sine inverse of the given value; the returned angle is in the range-pi/2 through pi/2.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    atan(col)\n",
      "        Computes the tangent inverse of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    atan2(col1, col2)\n",
      "        Returns the angle theta from the conversion of rectangular coordinates (x, y) topolar coordinates (r, theta).\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    avg(col)\n",
      "        Aggregate function: returns the average of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    base64(col)\n",
      "        Computes the BASE64 encoding of a binary column and returns it as a string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    bin(col)\n",
      "        Returns the string representation of the binary value of the given column.\n",
      "        \n",
      "        >>> df.select(bin(df.age).alias('c')).collect()\n",
      "        [Row(c='10'), Row(c='101')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    bitwiseNOT(col)\n",
      "        Computes bitwise not.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    broadcast(df)\n",
      "        Marks a DataFrame as small enough for use in broadcast joins.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    bround(col, scale=0)\n",
      "        Round the given value to `scale` decimal places using HALF_EVEN rounding mode if `scale` >= 0\n",
      "        or at integral part when `scale` < 0.\n",
      "        \n",
      "        >>> spark.createDataFrame([(2.5,)], ['a']).select(bround('a', 0).alias('r')).collect()\n",
      "        [Row(r=2.0)]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    cbrt(col)\n",
      "        Computes the cube-root of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    ceil(col)\n",
      "        Computes the ceiling of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    coalesce(*cols)\n",
      "        Returns the first column that is not null.\n",
      "        \n",
      "        >>> cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n",
      "        >>> cDf.show()\n",
      "        +----+----+\n",
      "        |   a|   b|\n",
      "        +----+----+\n",
      "        |null|null|\n",
      "        |   1|null|\n",
      "        |null|   2|\n",
      "        +----+----+\n",
      "        \n",
      "        >>> cDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()\n",
      "        +--------------+\n",
      "        |coalesce(a, b)|\n",
      "        +--------------+\n",
      "        |          null|\n",
      "        |             1|\n",
      "        |             2|\n",
      "        +--------------+\n",
      "        \n",
      "        >>> cDf.select('*', coalesce(cDf[\"a\"], lit(0.0))).show()\n",
      "        +----+----+----------------+\n",
      "        |   a|   b|coalesce(a, 0.0)|\n",
      "        +----+----+----------------+\n",
      "        |null|null|             0.0|\n",
      "        |   1|null|             1.0|\n",
      "        |null|   2|             0.0|\n",
      "        +----+----+----------------+\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    col(col)\n",
      "        Returns a :class:`Column` based on the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    collect_list(col)\n",
      "        Aggregate function: returns a list of objects with duplicates.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    collect_set(col)\n",
      "        Aggregate function: returns a set of objects with duplicate elements eliminated.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    column(col)\n",
      "        Returns a :class:`Column` based on the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    concat(*cols)\n",
      "        Concatenates multiple input string columns together into a single string column.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "        >>> df.select(concat(df.s, df.d).alias('s')).collect()\n",
      "        [Row(s='abcd123')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    concat_ws(sep, *cols)\n",
      "        Concatenates multiple input string columns together into a single string column,\n",
      "        using the given separator.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "        >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\n",
      "        [Row(s='abcd-123')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    conv(col, fromBase, toBase)\n",
      "        Convert a number in a string column from one base to another.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"010101\",)], ['n'])\n",
      "        >>> df.select(conv(df.n, 2, 16).alias('hex')).collect()\n",
      "        [Row(hex='15')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    corr(col1, col2)\n",
      "        Returns a new :class:`Column` for the Pearson Correlation Coefficient for ``col1``\n",
      "        and ``col2``.\n",
      "        \n",
      "        >>> a = range(20)\n",
      "        >>> b = [2 * x for x in range(20)]\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(corr(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=1.0)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    cos(col)\n",
      "        Computes the cosine of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    cosh(col)\n",
      "        Computes the hyperbolic cosine of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    count(col)\n",
      "        Aggregate function: returns the number of items in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    countDistinct(col, *cols)\n",
      "        Returns a new :class:`Column` for distinct count of ``col`` or ``cols``.\n",
      "        \n",
      "        >>> df.agg(countDistinct(df.age, df.name).alias('c')).collect()\n",
      "        [Row(c=2)]\n",
      "        \n",
      "        >>> df.agg(countDistinct(\"age\", \"name\").alias('c')).collect()\n",
      "        [Row(c=2)]\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    covar_pop(col1, col2)\n",
      "        Returns a new :class:`Column` for the population covariance of ``col1``\n",
      "        and ``col2``.\n",
      "        \n",
      "        >>> a = [1] * 10\n",
      "        >>> b = [1] * 10\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(covar_pop(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=0.0)]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    covar_samp(col1, col2)\n",
      "        Returns a new :class:`Column` for the sample covariance of ``col1``\n",
      "        and ``col2``.\n",
      "        \n",
      "        >>> a = [1] * 10\n",
      "        >>> b = [1] * 10\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(covar_samp(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=0.0)]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    crc32(col)\n",
      "        Calculates the cyclic redundancy check value  (CRC32) of a binary column and\n",
      "        returns the value as a bigint.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(crc32('a').alias('crc32')).collect()\n",
      "        [Row(crc32=2743272264)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    create_map(*cols)\n",
      "        Creates a new map column.\n",
      "        \n",
      "        :param cols: list of column names (string) or list of :class:`Column` expressions that grouped\n",
      "            as key-value pairs, e.g. (key1, value1, key2, value2, ...).\n",
      "        \n",
      "        >>> df.select(create_map('name', 'age').alias(\"map\")).collect()\n",
      "        [Row(map={'Alice': 2}), Row(map={'Bob': 5})]\n",
      "        >>> df.select(create_map([df.name, df.age]).alias(\"map\")).collect()\n",
      "        [Row(map={'Alice': 2}), Row(map={'Bob': 5})]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    cume_dist()\n",
      "        Window function: returns the cumulative distribution of values within a window partition,\n",
      "        i.e. the fraction of rows that are below the current row.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    current_date()\n",
      "        Returns the current date as a date column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    current_timestamp()\n",
      "        Returns the current timestamp as a timestamp column.\n",
      "    \n",
      "    date_add(start, days)\n",
      "        Returns the date that is `days` days after `start`\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['d'])\n",
      "        >>> df.select(date_add(df.d, 1).alias('d')).collect()\n",
      "        [Row(d=datetime.date(2015, 4, 9))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    date_format(date, format)\n",
      "        Converts a date/timestamp/string to a value of string in the format specified by the date\n",
      "        format given by the second argument.\n",
      "        \n",
      "        A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n",
      "        pattern letters of the Java class `java.text.SimpleDateFormat` can be used.\n",
      "        \n",
      "        .. note:: Use when ever possible specialized functions like `year`. These benefit from a\n",
      "            specialized implementation.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
      "        >>> df.select(date_format('a', 'MM/dd/yyy').alias('date')).collect()\n",
      "        [Row(date='04/08/2015')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    date_sub(start, days)\n",
      "        Returns the date that is `days` days before `start`\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['d'])\n",
      "        >>> df.select(date_sub(df.d, 1).alias('d')).collect()\n",
      "        [Row(d=datetime.date(2015, 4, 7))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    datediff(end, start)\n",
      "        Returns the number of days from `start` to `end`.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
      "        >>> df.select(datediff(df.d2, df.d1).alias('diff')).collect()\n",
      "        [Row(diff=32)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    dayofmonth(col)\n",
      "        Extract the day of the month of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
      "        >>> df.select(dayofmonth('a').alias('day')).collect()\n",
      "        [Row(day=8)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    dayofyear(col)\n",
      "        Extract the day of the year of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
      "        >>> df.select(dayofyear('a').alias('day')).collect()\n",
      "        [Row(day=98)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    decode(col, charset)\n",
      "        Computes the first argument into a string from a binary using the provided character set\n",
      "        (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    degrees(col)\n",
      "        Converts an angle measured in radians to an approximately equivalent angle measured in degrees.\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    dense_rank()\n",
      "        Window function: returns the rank of rows within a window partition, without any gaps.\n",
      "        \n",
      "        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n",
      "        sequence when there are ties. That is, if you were ranking a competition using dense_rank\n",
      "        and had three people tie for second place, you would say that all three were in second\n",
      "        place and that the next person came in third. Rank would give me sequential numbers, making\n",
      "        the person that came in third place (after the ties) would register as coming in fifth.\n",
      "        \n",
      "        This is equivalent to the DENSE_RANK function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    desc(col)\n",
      "        Returns a sort expression based on the descending order of the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    encode(col, charset)\n",
      "        Computes the first argument into a binary from a string using the provided character set\n",
      "        (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    exp(col)\n",
      "        Computes the exponential of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    explode(col)\n",
      "        Returns a new row for each element in the given array or map.\n",
      "        \n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
      "        >>> eDF.select(explode(eDF.intlist).alias(\"anInt\")).collect()\n",
      "        [Row(anInt=1), Row(anInt=2), Row(anInt=3)]\n",
      "        \n",
      "        >>> eDF.select(explode(eDF.mapfield).alias(\"key\", \"value\")).show()\n",
      "        +---+-----+\n",
      "        |key|value|\n",
      "        +---+-----+\n",
      "        |  a|    b|\n",
      "        +---+-----+\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    expm1(col)\n",
      "        Computes the exponential of the given value minus one.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    expr(str)\n",
      "        Parses the expression string into the column that it represents\n",
      "        \n",
      "        >>> df.select(expr(\"length(name)\")).collect()\n",
      "        [Row(length(name)=5), Row(length(name)=3)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    factorial(col)\n",
      "        Computes the factorial of the given value.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(5,)], ['n'])\n",
      "        >>> df.select(factorial(df.n).alias('f')).collect()\n",
      "        [Row(f=120)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    first(col, ignorenulls=False)\n",
      "        Aggregate function: returns the first value in a group.\n",
      "        \n",
      "        The function by default returns the first values it sees. It will return the first non-null\n",
      "        value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    floor(col)\n",
      "        Computes the floor of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    format_number(col, d)\n",
      "        Formats the number X to a format like '#,--#,--#.--', rounded to d decimal places\n",
      "        with HALF_EVEN round mode, and returns the result as a string.\n",
      "        \n",
      "        :param col: the column name of the numeric value to be formatted\n",
      "        :param d: the N decimal places\n",
      "        \n",
      "        >>> spark.createDataFrame([(5,)], ['a']).select(format_number('a', 4).alias('v')).collect()\n",
      "        [Row(v='5.0000')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    format_string(format, *cols)\n",
      "        Formats the arguments in printf-style and returns the result as a string column.\n",
      "        \n",
      "        :param col: the column name of the numeric value to be formatted\n",
      "        :param d: the N decimal places\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(5, \"hello\")], ['a', 'b'])\n",
      "        >>> df.select(format_string('%d %s', df.a, df.b).alias('v')).collect()\n",
      "        [Row(v='5 hello')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    from_json(col, schema, options={})\n",
      "        Parses a column containing a JSON string into a [[StructType]] or [[ArrayType]]\n",
      "        of [[StructType]]s with the specified schema. Returns `null`, in the case of an unparseable\n",
      "        string.\n",
      "        \n",
      "        :param col: string column in json format\n",
      "        :param schema: a StructType or ArrayType of StructType to use when parsing the json column\n",
      "        :param options: options to control parsing. accepts the same options as the json datasource\n",
      "        \n",
      "        >>> from pyspark.sql.types import *\n",
      "        >>> data = [(1, '''{\"a\": 1}''')]\n",
      "        >>> schema = StructType([StructField(\"a\", IntegerType())])\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=1))]\n",
      "        >>> data = [(1, '''[{\"a\": 1}]''')]\n",
      "        >>> schema = ArrayType(StructType([StructField(\"a\", IntegerType())]))\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=[Row(a=1)])]\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    from_unixtime(timestamp, format='yyyy-MM-dd HH:mm:ss')\n",
      "        Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string\n",
      "        representing the timestamp of that moment in the current system time zone in the given\n",
      "        format.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    from_utc_timestamp(timestamp, tz)\n",
      "        Given a timestamp, which corresponds to a certain time of day in UTC, returns another timestamp\n",
      "        that corresponds to the same time of day in the given timezone.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(from_utc_timestamp(df.t, \"PST\").alias('t')).collect()\n",
      "        [Row(t=datetime.datetime(1997, 2, 28, 2, 30))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    get_json_object(col, path)\n",
      "        Extracts json object from a json string based on json path specified, and returns json string\n",
      "        of the extracted json object. It will return null if the input json string is invalid.\n",
      "        \n",
      "        :param col: string column in json format\n",
      "        :param path: path to the json object to extract\n",
      "        \n",
      "        >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
      "        >>> df.select(df.key, get_json_object(df.jstring, '$.f1').alias(\"c0\"), \\\n",
      "        ...                   get_json_object(df.jstring, '$.f2').alias(\"c1\") ).collect()\n",
      "        [Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    greatest(*cols)\n",
      "        Returns the greatest value of the list of column names, skipping null values.\n",
      "        This function takes at least 2 parameters. It will return null iff all parameters are null.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
      "        >>> df.select(greatest(df.a, df.b, df.c).alias(\"greatest\")).collect()\n",
      "        [Row(greatest=4)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    grouping(col)\n",
      "        Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated\n",
      "        or not, returns 1 for aggregated or 0 for not aggregated in the result set.\n",
      "        \n",
      "        >>> df.cube(\"name\").agg(grouping(\"name\"), sum(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+--------------+--------+\n",
      "        | name|grouping(name)|sum(age)|\n",
      "        +-----+--------------+--------+\n",
      "        | null|             1|       7|\n",
      "        |Alice|             0|       2|\n",
      "        |  Bob|             0|       5|\n",
      "        +-----+--------------+--------+\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    grouping_id(*cols)\n",
      "        Aggregate function: returns the level of grouping, equals to\n",
      "        \n",
      "           (grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + ... + grouping(cn)\n",
      "        \n",
      "        .. note:: The list of columns should match with grouping columns exactly, or empty (means all\n",
      "            the grouping columns).\n",
      "        \n",
      "        >>> df.cube(\"name\").agg(grouping_id(), sum(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+-------------+--------+\n",
      "        | name|grouping_id()|sum(age)|\n",
      "        +-----+-------------+--------+\n",
      "        | null|            1|       7|\n",
      "        |Alice|            0|       2|\n",
      "        |  Bob|            0|       5|\n",
      "        +-----+-------------+--------+\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    hash(*cols)\n",
      "        Calculates the hash code of given columns, and returns the result as an int column.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(hash('a').alias('hash')).collect()\n",
      "        [Row(hash=-757602832)]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    hex(col)\n",
      "        Computes hex value of the given column, which could be :class:`pyspark.sql.types.StringType`,\n",
      "        :class:`pyspark.sql.types.BinaryType`, :class:`pyspark.sql.types.IntegerType` or\n",
      "        :class:`pyspark.sql.types.LongType`.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC', 3)], ['a', 'b']).select(hex('a'), hex('b')).collect()\n",
      "        [Row(hex(a)='414243', hex(b)='3')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    hour(col)\n",
      "        Extract the hours of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['a'])\n",
      "        >>> df.select(hour('a').alias('hour')).collect()\n",
      "        [Row(hour=13)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    hypot(col1, col2)\n",
      "        Computes ``sqrt(a^2 + b^2)`` without intermediate overflow or underflow.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    initcap(col)\n",
      "        Translate the first letter of each word to upper case in the sentence.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ab cd',)], ['a']).select(initcap(\"a\").alias('v')).collect()\n",
      "        [Row(v='Ab Cd')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    input_file_name()\n",
      "        Creates a string column for the file name of the current Spark task.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    instr(str, substr)\n",
      "        Locate the position of the first occurrence of substr column in the given string.\n",
      "        Returns null if either of the arguments are null.\n",
      "        \n",
      "        .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\n",
      "            could not be found in str.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(instr(df.s, 'b').alias('s')).collect()\n",
      "        [Row(s=2)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    isnan(col)\n",
      "        An expression that returns true iff the column is NaN.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
      "        >>> df.select(isnan(\"a\").alias(\"r1\"), isnan(df.a).alias(\"r2\")).collect()\n",
      "        [Row(r1=False, r2=False), Row(r1=True, r2=True)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    isnull(col)\n",
      "        An expression that returns true iff the column is null.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1, None), (None, 2)], (\"a\", \"b\"))\n",
      "        >>> df.select(isnull(\"a\").alias(\"r1\"), isnull(df.a).alias(\"r2\")).collect()\n",
      "        [Row(r1=False, r2=False), Row(r1=True, r2=True)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    json_tuple(col, *fields)\n",
      "        Creates a new row for a json column according to the given field names.\n",
      "        \n",
      "        :param col: string column in json format\n",
      "        :param fields: list of fields to extract\n",
      "        \n",
      "        >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
      "        >>> df.select(df.key, json_tuple(df.jstring, 'f1', 'f2')).collect()\n",
      "        [Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    kurtosis(col)\n",
      "        Aggregate function: returns the kurtosis of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    lag(col, count=1, default=None)\n",
      "        Window function: returns the value that is `offset` rows before the current row, and\n",
      "        `defaultValue` if there is less than `offset` rows before the current row. For example,\n",
      "        an `offset` of one will return the previous row at any given point in the window partition.\n",
      "        \n",
      "        This is equivalent to the LAG function in SQL.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        :param count: number of row to extend\n",
      "        :param default: default value\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    last(col, ignorenulls=False)\n",
      "        Aggregate function: returns the last value in a group.\n",
      "        \n",
      "        The function by default returns the last values it sees. It will return the last non-null\n",
      "        value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    last_day(date)\n",
      "        Returns the last day of the month which the given date belongs to.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-10',)], ['d'])\n",
      "        >>> df.select(last_day(df.d).alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    lead(col, count=1, default=None)\n",
      "        Window function: returns the value that is `offset` rows after the current row, and\n",
      "        `defaultValue` if there is less than `offset` rows after the current row. For example,\n",
      "        an `offset` of one will return the next row at any given point in the window partition.\n",
      "        \n",
      "        This is equivalent to the LEAD function in SQL.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        :param count: number of row to extend\n",
      "        :param default: default value\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    least(*cols)\n",
      "        Returns the least value of the list of column names, skipping null values.\n",
      "        This function takes at least 2 parameters. It will return null iff all parameters are null.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
      "        >>> df.select(least(df.a, df.b, df.c).alias(\"least\")).collect()\n",
      "        [Row(least=1)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    length(col)\n",
      "        Calculates the length of a string or binary expression.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(length('a').alias('length')).collect()\n",
      "        [Row(length=3)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    levenshtein(left, right)\n",
      "        Computes the Levenshtein distance of the two given strings.\n",
      "        \n",
      "        >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n",
      "        >>> df0.select(levenshtein('l', 'r').alias('d')).collect()\n",
      "        [Row(d=3)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    lit(col)\n",
      "        Creates a :class:`Column` of literal value.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    locate(substr, str, pos=1)\n",
      "        Locate the position of the first occurrence of substr in a string column, after position pos.\n",
      "        \n",
      "        .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\n",
      "            could not be found in str.\n",
      "        \n",
      "        :param substr: a string\n",
      "        :param str: a Column of :class:`pyspark.sql.types.StringType`\n",
      "        :param pos: start position (zero based)\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(locate('b', df.s, 1).alias('s')).collect()\n",
      "        [Row(s=2)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    log(arg1, arg2=None)\n",
      "        Returns the first argument-based logarithm of the second argument.\n",
      "        \n",
      "        If there is only one argument, then this takes the natural logarithm of the argument.\n",
      "        \n",
      "        >>> df.select(log(10.0, df.age).alias('ten')).rdd.map(lambda l: str(l.ten)[:7]).collect()\n",
      "        ['0.30102', '0.69897']\n",
      "        \n",
      "        >>> df.select(log(df.age).alias('e')).rdd.map(lambda l: str(l.e)[:7]).collect()\n",
      "        ['0.69314', '1.60943']\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    log10(col)\n",
      "        Computes the logarithm of the given value in Base 10.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    log1p(col)\n",
      "        Computes the natural logarithm of the given value plus one.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    log2(col)\n",
      "        Returns the base-2 logarithm of the argument.\n",
      "        \n",
      "        >>> spark.createDataFrame([(4,)], ['a']).select(log2('a').alias('log2')).collect()\n",
      "        [Row(log2=2.0)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    lower(col)\n",
      "        Converts a string column to lower case.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    lpad(col, len, pad)\n",
      "        Left-pad the string column to width `len` with `pad`.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(lpad(df.s, 6, '#').alias('s')).collect()\n",
      "        [Row(s='##abcd')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    ltrim(col)\n",
      "        Trim the spaces from left end for the specified string value.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    max(col)\n",
      "        Aggregate function: returns the maximum value of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    md5(col)\n",
      "        Calculates the MD5 digest and returns the value as a 32 character hex string.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(md5('a').alias('hash')).collect()\n",
      "        [Row(hash='902fbdd2b1df0c4f70b4a5d23525e932')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    mean(col)\n",
      "        Aggregate function: returns the average of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    min(col)\n",
      "        Aggregate function: returns the minimum value of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    minute(col)\n",
      "        Extract the minutes of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['a'])\n",
      "        >>> df.select(minute('a').alias('minute')).collect()\n",
      "        [Row(minute=8)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    monotonically_increasing_id()\n",
      "        A column that generates monotonically increasing 64-bit integers.\n",
      "        \n",
      "        The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.\n",
      "        The current implementation puts the partition ID in the upper 31 bits, and the record number\n",
      "        within each partition in the lower 33 bits. The assumption is that the data frame has\n",
      "        less than 1 billion partitions, and each partition has less than 8 billion records.\n",
      "        \n",
      "        As an example, consider a :class:`DataFrame` with two partitions, each with 3 records.\n",
      "        This expression would return the following IDs:\n",
      "        0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594.\n",
      "        \n",
      "        >>> df0 = sc.parallelize(range(2), 2).mapPartitions(lambda x: [(1,), (2,), (3,)]).toDF(['col1'])\n",
      "        >>> df0.select(monotonically_increasing_id().alias('id')).collect()\n",
      "        [Row(id=0), Row(id=1), Row(id=2), Row(id=8589934592), Row(id=8589934593), Row(id=8589934594)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    month(col)\n",
      "         Extract the month of a given date as integer.\n",
      "        \n",
      "         >>> df = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
      "         >>> df.select(month('a').alias('month')).collect()\n",
      "         [Row(month=4)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    months_between(date1, date2)\n",
      "        Returns the number of months between date1 and date2.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['t', 'd'])\n",
      "        >>> df.select(months_between(df.t, df.d).alias('months')).collect()\n",
      "        [Row(months=3.9495967...)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    nanvl(col1, col2)\n",
      "        Returns col1 if it is not NaN, or col2 if col1 is NaN.\n",
      "        \n",
      "        Both inputs should be floating point columns (DoubleType or FloatType).\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
      "        >>> df.select(nanvl(\"a\", \"b\").alias(\"r1\"), nanvl(df.a, df.b).alias(\"r2\")).collect()\n",
      "        [Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    next_day(date, dayOfWeek)\n",
      "        Returns the first date which is later than the value of the date column.\n",
      "        \n",
      "        Day of the week parameter is case insensitive, and accepts:\n",
      "            \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\".\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-07-27',)], ['d'])\n",
      "        >>> df.select(next_day(df.d, 'Sun').alias('date')).collect()\n",
      "        [Row(date=datetime.date(2015, 8, 2))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    ntile(n)\n",
      "        Window function: returns the ntile group id (from 1 to `n` inclusive)\n",
      "        in an ordered window partition. For example, if `n` is 4, the first\n",
      "        quarter of the rows will get value 1, the second quarter will get 2,\n",
      "        the third quarter will get 3, and the last quarter will get 4.\n",
      "        \n",
      "        This is equivalent to the NTILE function in SQL.\n",
      "        \n",
      "        :param n: an integer\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    percent_rank()\n",
      "        Window function: returns the relative rank (i.e. percentile) of rows within a window partition.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    posexplode(col)\n",
      "        Returns a new row for each element with position in the given array or map.\n",
      "        \n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
      "        >>> eDF.select(posexplode(eDF.intlist)).collect()\n",
      "        [Row(pos=0, col=1), Row(pos=1, col=2), Row(pos=2, col=3)]\n",
      "        \n",
      "        >>> eDF.select(posexplode(eDF.mapfield)).show()\n",
      "        +---+---+-----+\n",
      "        |pos|key|value|\n",
      "        +---+---+-----+\n",
      "        |  0|  a|    b|\n",
      "        +---+---+-----+\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    pow(col1, col2)\n",
      "        Returns the value of the first argument raised to the power of the second argument.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    quarter(col)\n",
      "        Extract the quarter of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
      "        >>> df.select(quarter('a').alias('quarter')).collect()\n",
      "        [Row(quarter=2)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    radians(col)\n",
      "        Converts an angle measured in degrees to an approximately equivalent angle measured in radians.\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    rand(seed=None)\n",
      "        Generates a random column with independent and identically distributed (i.i.d.) samples\n",
      "        from U[0.0, 1.0].\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    randn(seed=None)\n",
      "        Generates a column with independent and identically distributed (i.i.d.) samples from\n",
      "        the standard normal distribution.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    rank()\n",
      "        Window function: returns the rank of rows within a window partition.\n",
      "        \n",
      "        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n",
      "        sequence when there are ties. That is, if you were ranking a competition using dense_rank\n",
      "        and had three people tie for second place, you would say that all three were in second\n",
      "        place and that the next person came in third. Rank would give me sequential numbers, making\n",
      "        the person that came in third place (after the ties) would register as coming in fifth.\n",
      "        \n",
      "        This is equivalent to the RANK function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    regexp_extract(str, pattern, idx)\n",
      "        Extract a specific group matched by a Java regex, from the specified string column.\n",
      "        If the regex did not match, or the specified group did not match, an empty string is returned.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('100-200',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', '(\\d+)-(\\d+)', 1).alias('d')).collect()\n",
      "        [Row(d='100')]\n",
      "        >>> df = spark.createDataFrame([('foo',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', '(\\d+)', 1).alias('d')).collect()\n",
      "        [Row(d='')]\n",
      "        >>> df = spark.createDataFrame([('aaaac',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', '(a+)(b)?(c)', 2).alias('d')).collect()\n",
      "        [Row(d='')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    regexp_replace(str, pattern, replacement)\n",
      "        Replace all substrings of the specified string value that match regexp with rep.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('100-200',)], ['str'])\n",
      "        >>> df.select(regexp_replace('str', '(\\d+)', '--').alias('d')).collect()\n",
      "        [Row(d='-----')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    repeat(col, n)\n",
      "        Repeats a string column n times, and returns it as a new string column.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('ab',)], ['s',])\n",
      "        >>> df.select(repeat(df.s, 3).alias('s')).collect()\n",
      "        [Row(s='ababab')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    reverse(col)\n",
      "        Reverses the string column and returns it as a new string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    rint(col)\n",
      "        Returns the double value that is closest in value to the argument and is equal to a mathematical integer.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    round(col, scale=0)\n",
      "        Round the given value to `scale` decimal places using HALF_UP rounding mode if `scale` >= 0\n",
      "        or at integral part when `scale` < 0.\n",
      "        \n",
      "        >>> spark.createDataFrame([(2.5,)], ['a']).select(round('a', 0).alias('r')).collect()\n",
      "        [Row(r=3.0)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    row_number()\n",
      "        Window function: returns a sequential number starting at 1 within a window partition.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    rpad(col, len, pad)\n",
      "        Right-pad the string column to width `len` with `pad`.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(rpad(df.s, 6, '#').alias('s')).collect()\n",
      "        [Row(s='abcd##')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    rtrim(col)\n",
      "        Trim the spaces from right end for the specified string value.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    second(col)\n",
      "        Extract the seconds of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08 13:08:15',)], ['a'])\n",
      "        >>> df.select(second('a').alias('second')).collect()\n",
      "        [Row(second=15)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    sha1(col)\n",
      "        Returns the hex string result of SHA-1.\n",
      "        \n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(sha1('a').alias('hash')).collect()\n",
      "        [Row(hash='3c01bdbb26f358bab27f267924aa2c9a03fcfdb8')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    sha2(col, numBits)\n",
      "        Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384,\n",
      "        and SHA-512). The numBits indicates the desired bit length of the result, which must have a\n",
      "        value of 224, 256, 384, 512, or 0 (which is equivalent to 256).\n",
      "        \n",
      "        >>> digests = df.select(sha2(df.name, 256).alias('s')).collect()\n",
      "        >>> digests[0]\n",
      "        Row(s='3bc51062973c458d5a6f2d8d64a023246354ad7e064b1e4e009ec8a0699a3043')\n",
      "        >>> digests[1]\n",
      "        Row(s='cd9fb1e148ccd8442e5aa74904cc73bf6fb54d1d54d333bd596aa9bb4bb4e961')\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    shiftLeft(col, numBits)\n",
      "        Shift the given value numBits left.\n",
      "        \n",
      "        >>> spark.createDataFrame([(21,)], ['a']).select(shiftLeft('a', 1).alias('r')).collect()\n",
      "        [Row(r=42)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    shiftRight(col, numBits)\n",
      "        (Signed) shift the given value numBits right.\n",
      "        \n",
      "        >>> spark.createDataFrame([(42,)], ['a']).select(shiftRight('a', 1).alias('r')).collect()\n",
      "        [Row(r=21)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    shiftRightUnsigned(col, numBits)\n",
      "        Unsigned shift the given value numBits right.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(-42,)], ['a'])\n",
      "        >>> df.select(shiftRightUnsigned('a', 1).alias('r')).collect()\n",
      "        [Row(r=9223372036854775787)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    signum(col)\n",
      "        Computes the signum of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    sin(col)\n",
      "        Computes the sine of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    sinh(col)\n",
      "        Computes the hyperbolic sine of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    size(col)\n",
      "        Collection function: returns the length of the array or map stored in the column.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([1, 2, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(size(df.data)).collect()\n",
      "        [Row(size(data)=3), Row(size(data)=1), Row(size(data)=0)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    skewness(col)\n",
      "        Aggregate function: returns the skewness of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    sort_array(col, asc=True)\n",
      "        Collection function: sorts the input array in ascending or descending order according\n",
      "        to the natural ordering of the array elements.\n",
      "        \n",
      "        :param col: name of column or expression\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(sort_array(df.data).alias('r')).collect()\n",
      "        [Row(r=[1, 2, 3]), Row(r=[1]), Row(r=[])]\n",
      "        >>> df.select(sort_array(df.data, asc=False).alias('r')).collect()\n",
      "        [Row(r=[3, 2, 1]), Row(r=[1]), Row(r=[])]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    soundex(col)\n",
      "        Returns the SoundEx encoding for a string\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"Peters\",),(\"Uhrbach\",)], ['name'])\n",
      "        >>> df.select(soundex(df.name).alias(\"soundex\")).collect()\n",
      "        [Row(soundex='P362'), Row(soundex='U612')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    spark_partition_id()\n",
      "        A column for partition ID.\n",
      "        \n",
      "        .. note:: This is indeterministic because it depends on data partitioning and task scheduling.\n",
      "        \n",
      "        >>> df.repartition(1).select(spark_partition_id().alias(\"pid\")).collect()\n",
      "        [Row(pid=0), Row(pid=0)]\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    split(str, pattern)\n",
      "        Splits str around pattern (pattern is a regular expression).\n",
      "        \n",
      "        .. note:: pattern is a string represent the regular expression.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('ab12cd',)], ['s',])\n",
      "        >>> df.select(split(df.s, '[0-9]+').alias('s')).collect()\n",
      "        [Row(s=['ab', 'cd'])]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    sqrt(col)\n",
      "        Computes the square root of the specified float value.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    stddev(col)\n",
      "        Aggregate function: returns the unbiased sample standard deviation of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    stddev_pop(col)\n",
      "        Aggregate function: returns population standard deviation of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    stddev_samp(col)\n",
      "        Aggregate function: returns the unbiased sample standard deviation of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    struct(*cols)\n",
      "        Creates a new struct column.\n",
      "        \n",
      "        :param cols: list of column names (string) or list of :class:`Column` expressions\n",
      "        \n",
      "        >>> df.select(struct('age', 'name').alias(\"struct\")).collect()\n",
      "        [Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]\n",
      "        >>> df.select(struct([df.age, df.name]).alias(\"struct\")).collect()\n",
      "        [Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    substring(str, pos, len)\n",
      "        Substring starts at `pos` and is of length `len` when str is String type or\n",
      "        returns the slice of byte array that starts at `pos` in byte and is of length `len`\n",
      "        when str is Binary type\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n",
      "        [Row(s='ab')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    substring_index(str, delim, count)\n",
      "        Returns the substring from string str before count occurrences of the delimiter delim.\n",
      "        If count is positive, everything the left of the final delimiter (counting from left) is\n",
      "        returned. If count is negative, every to the right of the final delimiter (counting from the\n",
      "        right) is returned. substring_index performs a case-sensitive match when searching for delim.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('a.b.c.d',)], ['s'])\n",
      "        >>> df.select(substring_index(df.s, '.', 2).alias('s')).collect()\n",
      "        [Row(s='a.b')]\n",
      "        >>> df.select(substring_index(df.s, '.', -3).alias('s')).collect()\n",
      "        [Row(s='b.c.d')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    sum(col)\n",
      "        Aggregate function: returns the sum of all values in the expression.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    sumDistinct(col)\n",
      "        Aggregate function: returns the sum of distinct values in the expression.\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    tan(col)\n",
      "        Computes the tangent of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    tanh(col)\n",
      "        Computes the hyperbolic tangent of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    toDegrees(col)\n",
      "        .. note:: Deprecated in 2.1, use degrees instead.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    toRadians(col)\n",
      "        .. note:: Deprecated in 2.1, use radians instead.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    to_date(col, format=None)\n",
      "        Converts a :class:`Column` of :class:`pyspark.sql.types.StringType` or\n",
      "        :class:`pyspark.sql.types.TimestampType` into :class:`pyspark.sql.types.DateType`\n",
      "        using the optionally specified format. Default format is 'yyyy-MM-dd'.\n",
      "        Specify formats according to\n",
      "        `SimpleDateFormats <http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html>`_.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_date(df.t).alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "        \n",
      "        .. versionadded:: 2.2\n",
      "    \n",
      "    to_json(col, options={})\n",
      "        Converts a column containing a [[StructType]] or [[ArrayType]] of [[StructType]]s into a\n",
      "        JSON string. Throws an exception, in the case of an unsupported type.\n",
      "        \n",
      "        :param col: name of column containing the struct or array of the structs\n",
      "        :param options: options to control converting. accepts the same options as the json datasource\n",
      "        \n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> from pyspark.sql.types import *\n",
      "        >>> data = [(1, Row(name='Alice', age=2))]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='{\"age\":2,\"name\":\"Alice\"}')]\n",
      "        >>> data = [(1, [Row(name='Alice', age=2), Row(name='Bob', age=3)])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[{\"age\":2,\"name\":\"Alice\"},{\"age\":3,\"name\":\"Bob\"}]')]\n",
      "        \n",
      "        .. versionadded:: 2.1\n",
      "    \n",
      "    to_timestamp(col, format=None)\n",
      "        Converts a :class:`Column` of :class:`pyspark.sql.types.StringType` or\n",
      "        :class:`pyspark.sql.types.TimestampType` into :class:`pyspark.sql.types.DateType`\n",
      "        using the optionally specified format. Default format is 'yyyy-MM-dd HH:mm:ss'. Specify\n",
      "        formats according to\n",
      "        `SimpleDateFormats <http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html>`_.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_timestamp(df.t).alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "        \n",
      "        .. versionadded:: 2.2\n",
      "    \n",
      "    to_utc_timestamp(timestamp, tz)\n",
      "        Given a timestamp, which corresponds to a certain time of day in the given timezone, returns\n",
      "        another timestamp that corresponds to the same time of day in UTC.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_utc_timestamp(df.t, \"PST\").alias('t')).collect()\n",
      "        [Row(t=datetime.datetime(1997, 2, 28, 18, 30))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    translate(srcCol, matching, replace)\n",
      "        A function translate any character in the `srcCol` by a character in `matching`.\n",
      "        The characters in `replace` is corresponding to the characters in `matching`.\n",
      "        The translate will happen when any character in the string matching with the character\n",
      "        in the `matching`.\n",
      "        \n",
      "        >>> spark.createDataFrame([('translate',)], ['a']).select(translate('a', \"rnlt\", \"123\") \\\n",
      "        ...     .alias('r')).collect()\n",
      "        [Row(r='1a2s3ae')]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    trim(col)\n",
      "        Trim the spaces from both ends for the specified string column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    trunc(date, format)\n",
      "        Returns date truncated to the unit specified by the format.\n",
      "        \n",
      "        :param format: 'year', 'YYYY', 'yy' or 'month', 'mon', 'mm'\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28',)], ['d'])\n",
      "        >>> df.select(trunc(df.d, 'year').alias('year')).collect()\n",
      "        [Row(year=datetime.date(1997, 1, 1))]\n",
      "        >>> df.select(trunc(df.d, 'mon').alias('month')).collect()\n",
      "        [Row(month=datetime.date(1997, 2, 1))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    udf(f=None, returnType=StringType)\n",
      "        Creates a :class:`Column` expression representing a user defined function (UDF).\n",
      "        \n",
      "        .. note:: The user-defined functions must be deterministic. Due to optimization,\n",
      "            duplicate invocations may be eliminated or the function may even be invoked more times than\n",
      "            it is present in the query.\n",
      "        \n",
      "        :param f: python function if used as a standalone function\n",
      "        :param returnType: a :class:`pyspark.sql.types.DataType` object\n",
      "        \n",
      "        >>> from pyspark.sql.types import IntegerType\n",
      "        >>> slen = udf(lambda s: len(s), IntegerType())\n",
      "        >>> @udf\n",
      "        ... def to_upper(s):\n",
      "        ...     if s is not None:\n",
      "        ...         return s.upper()\n",
      "        ...\n",
      "        >>> @udf(returnType=IntegerType())\n",
      "        ... def add_one(x):\n",
      "        ...     if x is not None:\n",
      "        ...         return x + 1\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
      "        >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\n",
      "        +----------+--------------+------------+\n",
      "        |slen(name)|to_upper(name)|add_one(age)|\n",
      "        +----------+--------------+------------+\n",
      "        |         8|      JOHN DOE|          22|\n",
      "        +----------+--------------+------------+\n",
      "        \n",
      "        .. versionadded:: 1.3\n",
      "    \n",
      "    unbase64(col)\n",
      "        Decodes a BASE64 encoded string column and returns it as a binary column.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    unhex(col)\n",
      "        Inverse of hex. Interprets each pair of characters as a hexadecimal number\n",
      "        and converts to the byte representation of number.\n",
      "        \n",
      "        >>> spark.createDataFrame([('414243',)], ['a']).select(unhex('a')).collect()\n",
      "        [Row(unhex(a)=bytearray(b'ABC'))]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    unix_timestamp(timestamp=None, format='yyyy-MM-dd HH:mm:ss')\n",
      "        Convert time string with given pattern ('yyyy-MM-dd HH:mm:ss', by default)\n",
      "        to Unix time stamp (in seconds), using the default timezone and the default\n",
      "        locale, return null if fail.\n",
      "        \n",
      "        if `timestamp` is None, then it returns current timestamp.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    upper(col)\n",
      "        Converts a string column to upper case.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    var_pop(col)\n",
      "        Aggregate function: returns the population variance of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    var_samp(col)\n",
      "        Aggregate function: returns the unbiased variance of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    variance(col)\n",
      "        Aggregate function: returns the population variance of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6\n",
      "    \n",
      "    weekofyear(col)\n",
      "        Extract the week number of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
      "        >>> df.select(weekofyear(df.a).alias('week')).collect()\n",
      "        [Row(week=15)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "    \n",
      "    when(condition, value)\n",
      "        Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\n",
      "        \n",
      "        :param condition: a boolean :class:`Column` expression.\n",
      "        :param value: a literal value, or a :class:`Column` expression.\n",
      "        \n",
      "        >>> df.select(when(df['age'] == 2, 3).otherwise(4).alias(\"age\")).collect()\n",
      "        [Row(age=3), Row(age=4)]\n",
      "        \n",
      "        >>> df.select(when(df.age == 2, df.age + 1).alias(\"age\")).collect()\n",
      "        [Row(age=3), Row(age=None)]\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "    \n",
      "    window(timeColumn, windowDuration, slideDuration=None, startTime=None)\n",
      "        Bucketize rows into one or more time windows given a timestamp specifying column. Window\n",
      "        starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window\n",
      "        [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in\n",
      "        the order of months are not supported.\n",
      "        \n",
      "        The time column must be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        Durations are provided as strings, e.g. '1 second', '1 day 12 hours', '2 minutes'. Valid\n",
      "        interval strings are 'week', 'day', 'hour', 'minute', 'second', 'millisecond', 'microsecond'.\n",
      "        If the ``slideDuration`` is not provided, the windows will be tumbling windows.\n",
      "        \n",
      "        The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start\n",
      "        window intervals. For example, in order to have hourly tumbling windows that start 15 minutes\n",
      "        past the hour, e.g. 12:15-13:15, 13:15-14:15... provide `startTime` as `15 minutes`.\n",
      "        \n",
      "        The output column will be a struct called 'window' by default with the nested columns 'start'\n",
      "        and 'end', where 'start' and 'end' will be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(\"2016-03-11 09:00:07\", 1)]).toDF(\"date\", \"val\")\n",
      "        >>> w = df.groupBy(window(\"date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\n",
      "        >>> w.select(w.window.start.cast(\"string\").alias(\"start\"),\n",
      "        ...          w.window.end.cast(\"string\").alias(\"end\"), \"sum\").collect()\n",
      "        [Row(start='2016-03-11 09:00:05', end='2016-03-11 09:00:10', sum=1)]\n",
      "        \n",
      "        .. versionadded:: 2.0\n",
      "    \n",
      "    year(col)\n",
      "        Extract the year of a given date as integer.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['a'])\n",
      "        >>> df.select(year('a').alias('year')).collect()\n",
      "        [Row(year=2015)]\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "\n",
      "DATA\n",
      "    __all__ = ['abs', 'acos', 'add_months', 'approxCountDistinct', 'approx...\n",
      "\n",
      "FILE\n",
      "    /usr/local/spark/python/pyspark/sql/functions.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions\n",
    "\n",
    "help(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not Column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-f3d725202b64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m newdf.select(\"id\",\n\u001b[1;32m      4\u001b[0m              \u001b[0;34m\"position\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m              math.sqrt(newdf[\"square\"]))\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not Column"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "newdf.select(\"id\",\n",
    "             \"position\",\n",
    "             math.sqrt(newdf[\"square\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, position: string, SQRT(square): double]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.select(\"id\",\n",
    "             \"position\",\n",
    "             functions.sqrt(\"square\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This errors out because \n",
    "\n",
    "```python\n",
    "math.log1p\n",
    "```\n",
    "\n",
    "is not a udf: it doesn't know how to work with strings or Column objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can transform it into a udf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same with any function we dream up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_truth = functions.udf(lambda field: \"awesome\" if field == \"data scientist\" else \"loser\")\n",
    "da_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, position: string, <lambda>(position): string]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"id\",\n",
    "          \"position\",\n",
    "          da_truth(\"position\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------+\n",
      "| id|       position|   amen|\n",
      "+---+---------------+-------+\n",
      "|  0|project manager|  loser|\n",
      "|  1|        teacher|  loser|\n",
      "|  2|        teacher|  loser|\n",
      "|  3|project manager|  loser|\n",
      "|  4| data scientist|awesome|\n",
      "|  5|        teacher|  loser|\n",
      "|  6|        teacher|  loser|\n",
      "|  7|        teacher|  loser|\n",
      "|  8|project manager|  loser|\n",
      "|  9|        teacher|  loser|\n",
      "| 10|project manager|  loser|\n",
      "| 11|project manager|  loser|\n",
      "+---+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cooldf = df.select(\"id\",\n",
    "                   \"position\",\n",
    "                   da_truth(\"position\").alias(\"amen\"))\n",
    "\n",
    "cooldf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want the resulting columns to be of a particular type, we need to specify the return type. This is because in Python return types can not be inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[<lambda>(id): string]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubler = functions.udf(lambda n: n + n)\n",
    "\n",
    "cooldf.select(doubler(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[<lambda>(id): int]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubler = functions.udf(lambda n: n + n, types.IntegerType())\n",
    "\n",
    "cooldf.select(doubler(\"id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: \n",
    "\n",
    "Create a 'salary' field in our df. make it 20000 for teachers, 40000 for product manager and 7e5 for data scientist.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------+\n",
      "| id|       position|   amen|\n",
      "+---+---------------+-------+\n",
      "|  0|project manager|  loser|\n",
      "|  1|        teacher|  loser|\n",
      "|  2|        teacher|  loser|\n",
      "|  3|project manager|  loser|\n",
      "|  4| data scientist|awesome|\n",
      "|  5|        teacher|  loser|\n",
      "|  6|        teacher|  loser|\n",
      "|  7|        teacher|  loser|\n",
      "|  8|project manager|  loser|\n",
      "|  9|        teacher|  loser|\n",
      "| 10|project manager|  loser|\n",
      "| 11|project manager|  loser|\n",
      "+---+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cooldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def salary(position):\n",
    "    \n",
    "    salary_table = {\"data scientist\" : 700000, \"project manager\" : 40000, \"teacher\" : 20000}\n",
    "        \n",
    "    return salary_table[position]\n",
    "\n",
    "salary(\"teacher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.salary>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_udf = functions.udf(salary)\n",
    "salary_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------+----------------+\n",
      "| id|       position|   amen|salary(position)|\n",
      "+---+---------------+-------+----------------+\n",
      "|  0|project manager|  loser|           40000|\n",
      "|  1|        teacher|  loser|           20000|\n",
      "|  2|        teacher|  loser|           20000|\n",
      "|  3|project manager|  loser|           40000|\n",
      "|  4| data scientist|awesome|          700000|\n",
      "|  5|        teacher|  loser|           20000|\n",
      "|  6|        teacher|  loser|           20000|\n",
      "|  7|        teacher|  loser|           20000|\n",
      "|  8|project manager|  loser|           40000|\n",
      "|  9|        teacher|  loser|           20000|\n",
      "| 10|project manager|  loser|           40000|\n",
      "| 11|project manager|  loser|           40000|\n",
      "+---+---------------+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exercise_result = cooldf.select(\"id\", \n",
    "                                \"position\",\n",
    "                                \"amen\",\n",
    "                                salary_udf(\"position\"))\n",
    "\n",
    "exercise_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- position: string (nullable = true)\n",
      " |-- amen: string (nullable = true)\n",
      " |-- salary(position): string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exercise_result.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a column that is not the desired type, we can convert it with `cast`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------+------+\n",
      "| id|       position|   amen|sueldo|\n",
      "+---+---------------+-------+------+\n",
      "|  0|project manager|  loser| 40000|\n",
      "|  1|        teacher|  loser| 20000|\n",
      "|  2|        teacher|  loser| 20000|\n",
      "|  3|project manager|  loser| 40000|\n",
      "|  4| data scientist|awesome|700000|\n",
      "|  5|        teacher|  loser| 20000|\n",
      "|  6|        teacher|  loser| 20000|\n",
      "|  7|        teacher|  loser| 20000|\n",
      "|  8|project manager|  loser| 40000|\n",
      "|  9|        teacher|  loser| 20000|\n",
      "| 10|project manager|  loser| 40000|\n",
      "| 11|project manager|  loser| 40000|\n",
      "+---+---------------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exercise_result = cooldf.select(\"id\", \n",
    "                                \"position\",\n",
    "                                \"amen\",\n",
    "                                salary_udf(\"position\").cast(types.IntegerType()).alias(\"sueldo\"))\n",
    "exercise_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- position: string (nullable = true)\n",
      " |-- amen: string (nullable = true)\n",
      " |-- sueldo: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exercise_result.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics\n",
    "\n",
    "https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.12091276966734668"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exercise_result.stat.corr(\"id\", \"sueldo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .crosstab()\n",
    "\n",
    "Crosstab returns the contingency table for two columns, as a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "La tabla de contingencia es una tabla donde se ven las relaciones de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc_udf = functions.udf(lambda: random.choice([\"Madrid\", \"Barcelona\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, position: string, amen: string, sueldo: int, location: string]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = exercise_result.withColumn(\"location\", loc_udf())\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------+------+---------+\n",
      "| id|       position|   amen|sueldo| location|\n",
      "+---+---------------+-------+------+---------+\n",
      "|  0|project manager|  loser| 40000|   Madrid|\n",
      "|  1|        teacher|  loser| 20000|   Madrid|\n",
      "|  2|        teacher|  loser| 20000|Barcelona|\n",
      "|  3|project manager|  loser| 40000|Barcelona|\n",
      "|  4| data scientist|awesome|700000|Barcelona|\n",
      "|  5|        teacher|  loser| 20000|Barcelona|\n",
      "|  6|        teacher|  loser| 20000|Barcelona|\n",
      "|  7|        teacher|  loser| 20000|Barcelona|\n",
      "|  8|project manager|  loser| 40000|Barcelona|\n",
      "|  9|        teacher|  loser| 20000|   Madrid|\n",
      "| 10|project manager|  loser| 40000|   Madrid|\n",
      "| 11|project manager|  loser| 40000|   Madrid|\n",
      "+---+---------------+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.cache()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+------+\n",
      "|position_location|Barcelona|Madrid|\n",
      "+-----------------+---------+------+\n",
      "|  project manager|        2|     3|\n",
      "|   data scientist|        1|     0|\n",
      "|          teacher|        4|     2|\n",
      "+-----------------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.crosstab(\"position\", \"location\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tabla de contingencias es para ver si dos variables son independientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping\n",
    "\n",
    "Grouping works very similarly to Pandas: executing groupby (or groupBy) on a DataFrame will return an object (a GroupedData) that can then be aggregated to obtain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7fead0624ef0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups = df2.groupby(\"location\")\n",
    "groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GroupedData has several aggregation functions defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "| location|       avg(sueldo)|\n",
      "+---------+------------------+\n",
      "|   Madrid|           32000.0|\n",
      "|Barcelona|122857.14285714286|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groups.avg(\"sueldo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do several aggregations in a single step, with a number of different syntaxes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+---------------+\n",
      "| location|       avg(sueldo)|  min(position)|\n",
      "+---------+------------------+---------------+\n",
      "|   Madrid|           32000.0|project manager|\n",
      "|Barcelona|122857.14285714286| data scientist|\n",
      "+---------+------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groups.agg({\"sueldo\" : \"mean\", \"position\" : \"min\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersections\n",
    "\n",
    "Veru much like SQL joins. We can specify the columns and the join method (left, right, inner, outer) or we can let Spark infer them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 5000, 'Sevilla'),\n",
       " (11, 10000, 'Madrid'),\n",
       " (9, 2000, 'Madrid'),\n",
       " (10, 0, 'Sevilla'),\n",
       " (9, 1000, 'Barcelona')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "data = list(zip(\n",
    "[10, 11, 9, 10, 9],\n",
    "    [5000, 10000, 2000, 0, 1000],\n",
    "    [random.choice([\"Madrid\", \"Barcelona\", \"Sevilla\"]) for _ in range(5)]))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+\n",
      "| id|bonus| location|\n",
      "+---+-----+---------+\n",
      "| 10| 5000|  Sevilla|\n",
      "| 11|10000|   Madrid|\n",
      "|  9| 2000|   Madrid|\n",
      "| 10|    0|  Sevilla|\n",
      "|  9| 1000|Barcelona|\n",
      "+---+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "right_df = session.createDataFrame(data, schema = [\"id\", \"bonus\", \"location\"])\n",
    "right_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Detected cartesian product for INNER join between logical plans\\nInMemoryRelation [id#51, position#52, amen#305, sueldo#342, location#356], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\\n   +- *Project [id#51, position#52, pythonUDF0#373 AS amen#305, cast(pythonUDF1#374 as int) AS sueldo#342, pythonUDF2#375 AS location#356]\\n      +- BatchEvalPython [<lambda>(position#52), salary(position#52), <lambda>()], [id#51, position#52, pythonUDF0#373, pythonUDF1#374, pythonUDF2#375]\\n         +- Scan ExistingRDD[id#51,position#52]\\nand\\nLogicalRDD [id#637L, bonus#638L, location#639]\\nJoin condition is missing or trivial.\\nUse the CROSS JOIN syntax to allow cartesian products between these relations.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o406.showString.\n: org.apache.spark.sql.AnalysisException: Detected cartesian product for INNER join between logical plans\nInMemoryRelation [id#51, position#52, amen#305, sueldo#342, location#356], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n   +- *Project [id#51, position#52, pythonUDF0#373 AS amen#305, cast(pythonUDF1#374 as int) AS sueldo#342, pythonUDF2#375 AS location#356]\n      +- BatchEvalPython [<lambda>(position#52), salary(position#52), <lambda>()], [id#51, position#52, pythonUDF0#373, pythonUDF1#374, pythonUDF2#375]\n         +- Scan ExistingRDD[id#51,position#52]\nand\nLogicalRDD [id#637L, bonus#638L, location#639]\nJoin condition is missing or trivial.\nUse the CROSS JOIN syntax to allow cartesian products between these relations.;\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$20.applyOrElse(Optimizer.scala:1080)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$20.applyOrElse(Optimizer.scala:1077)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts.apply(Optimizer.scala:1077)\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts.apply(Optimizer.scala:1062)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:78)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:89)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:89)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:2832)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2153)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2366)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:245)\n\tat sun.reflect.GeneratedMethodAccessor79.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-a8dfd362b151>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \"\"\"\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Detected cartesian product for INNER join between logical plans\\nInMemoryRelation [id#51, position#52, amen#305, sueldo#342, location#356], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\\n   +- *Project [id#51, position#52, pythonUDF0#373 AS amen#305, cast(pythonUDF1#374 as int) AS sueldo#342, pythonUDF2#375 AS location#356]\\n      +- BatchEvalPython [<lambda>(position#52), salary(position#52), <lambda>()], [id#51, position#52, pythonUDF0#373, pythonUDF1#374, pythonUDF2#375]\\n         +- Scan ExistingRDD[id#51,position#52]\\nand\\nLogicalRDD [id#637L, bonus#638L, location#639]\\nJoin condition is missing or trivial.\\nUse the CROSS JOIN syntax to allow cartesian products between these relations.;'"
     ]
    }
   ],
   "source": [
    "df2.join(right_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark refuses to do cross joins by default. To perform them, we can \n",
    "\n",
    "a) Allow then explicitly:\n",
    "\n",
    "```python\n",
    "session.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "b) Specify the join criterion\n",
    "\n",
    "```python\n",
    "df4.join(new_df, on='id').show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-----+------+--------+-----+---------+\n",
      "| id|       position| amen|sueldo|location|bonus| location|\n",
      "+---+---------------+-----+------+--------+-----+---------+\n",
      "| 10|project manager|loser| 40000|  Madrid| 5000|  Sevilla|\n",
      "| 11|project manager|loser| 40000|  Madrid|10000|   Madrid|\n",
      "|  9|        teacher|loser| 20000|  Madrid| 2000|   Madrid|\n",
      "| 10|project manager|loser| 40000|  Madrid|    0|  Sevilla|\n",
      "|  9|        teacher|loser| 20000|  Madrid| 1000|Barcelona|\n",
      "+---+---------------+-----+------+--------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.join(right_df, on = \"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------------+-------+------+-----+\n",
      "| id| location|       position|   amen|sueldo|bonus|\n",
      "+---+---------+---------------+-------+------+-----+\n",
      "|  3|Barcelona|project manager|  loser| 40000| null|\n",
      "|  9|   Madrid|        teacher|  loser| 20000| 2000|\n",
      "|  5|Barcelona|        teacher|  loser| 20000| null|\n",
      "|  0|   Madrid|project manager|  loser| 40000| null|\n",
      "|  7|Barcelona|        teacher|  loser| 20000| null|\n",
      "|  6|Barcelona|        teacher|  loser| 20000| null|\n",
      "| 10|   Madrid|project manager|  loser| 40000| null|\n",
      "|  2|Barcelona|        teacher|  loser| 20000| null|\n",
      "| 11|   Madrid|project manager|  loser| 40000|10000|\n",
      "|  8|Barcelona|project manager|  loser| 40000| null|\n",
      "|  4|Barcelona| data scientist|awesome|700000| null|\n",
      "|  1|   Madrid|        teacher|  loser| 20000| null|\n",
      "+---+---------+---------------+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = df2.join(right_df, on = [\"id\", \"location\"], how = \"left\").cache()\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Digression\n",
    "\n",
    "We can monitor our running jobs and storage used at the Spark Web UI. We can get its url with sc.uiWebUrl.\n",
    "\n",
    "StorageLevels represent how our DataFrame is cached: we can save the results of the computation up to that point, so that if we process several times the same data only the subsequent steps will be recomputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://10.0.2.15:4040'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can erase it with `unpersist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, location: string, position: string, amen: string, sueldo: int, bonus: bigint]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Calculate the [z-score](http://www.statisticshowto.com/probability-and-statistics/z-score/) of each employee's salary for their location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Calculate the mean and std of salary for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+------------------+\n",
      "| location|        avg_sueldo|        std_sueldo|\n",
      "+---------+------------------+------------------+\n",
      "|   Madrid|           32000.0|10954.451150103323|\n",
      "|Barcelona|122857.14285714286|254670.65566559267|\n",
      "+---------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggregated = joined.groupby(\"location\").agg(functions.avg(\"sueldo\").alias(\"avg_sueldo\"),\n",
    "                                            functions.stddev(\"sueldo\").alias(\"std_sueldo\"))\n",
    "\n",
    "aggregated.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Annotate each employee with the stats corresponding to their location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------------+-------+------+-----+------------------+------------------+\n",
      "| location| id|       position|   amen|sueldo|bonus|        avg_sueldo|        std_sueldo|\n",
      "+---------+---+---------------+-------+------+-----+------------------+------------------+\n",
      "|   Madrid|  9|        teacher|  loser| 20000| 2000|           32000.0|10954.451150103323|\n",
      "|   Madrid|  0|project manager|  loser| 40000| null|           32000.0|10954.451150103323|\n",
      "|   Madrid| 10|project manager|  loser| 40000| null|           32000.0|10954.451150103323|\n",
      "|   Madrid| 11|project manager|  loser| 40000|10000|           32000.0|10954.451150103323|\n",
      "|   Madrid|  1|        teacher|  loser| 20000| null|           32000.0|10954.451150103323|\n",
      "|Barcelona|  3|project manager|  loser| 40000| null|122857.14285714286|254670.65566559267|\n",
      "|Barcelona|  5|        teacher|  loser| 20000| null|122857.14285714286|254670.65566559267|\n",
      "|Barcelona|  7|        teacher|  loser| 20000| null|122857.14285714286|254670.65566559267|\n",
      "|Barcelona|  6|        teacher|  loser| 20000| null|122857.14285714286|254670.65566559267|\n",
      "|Barcelona|  2|        teacher|  loser| 20000| null|122857.14285714286|254670.65566559267|\n",
      "|Barcelona|  8|project manager|  loser| 40000| null|122857.14285714286|254670.65566559267|\n",
      "|Barcelona|  4| data scientist|awesome|700000| null|122857.14285714286|254670.65566559267|\n",
      "+---------+---+---------------+-------+------+-----+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_2 = joined.join(aggregated, on = \"location\", how = \"left\")\n",
    "joined_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Calculate the z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------------+-------+------+-----+------------------+------------------+--------------------+\n",
      "| location| id|       position|   amen|sueldo|bonus|        avg_sueldo|        std_sueldo|             z_score|\n",
      "+---------+---+---------------+-------+------+-----+------------------+------------------+--------------------+\n",
      "|   Madrid|  9|        teacher|  loser| 20000| 2000|           32000.0|10954.451150103323| -1.0954451150103321|\n",
      "|   Madrid|  0|project manager|  loser| 40000| null|           32000.0|10954.451150103323|  0.7302967433402214|\n",
      "|   Madrid| 10|project manager|  loser| 40000| null|           32000.0|10954.451150103323|  0.7302967433402214|\n",
      "|   Madrid| 11|project manager|  loser| 40000|10000|           32000.0|10954.451150103323|  0.7302967433402214|\n",
      "|   Madrid|  1|        teacher|  loser| 20000| null|           32000.0|10954.451150103323| -1.0954451150103321|\n",
      "|Barcelona|  3|project manager|  loser| 40000| null|122857.14285714286|254670.65566559267|-0.32535017684150597|\n",
      "|Barcelona|  5|        teacher|  loser| 20000| null|122857.14285714286|254670.65566559267| -0.4038829781480764|\n",
      "|Barcelona|  7|        teacher|  loser| 20000| null|122857.14285714286|254670.65566559267| -0.4038829781480764|\n",
      "|Barcelona|  6|        teacher|  loser| 20000| null|122857.14285714286|254670.65566559267| -0.4038829781480764|\n",
      "|Barcelona|  2|        teacher|  loser| 20000| null|122857.14285714286|254670.65566559267| -0.4038829781480764|\n",
      "|Barcelona|  8|project manager|  loser| 40000| null|122857.14285714286|254670.65566559267|-0.32535017684150597|\n",
      "|Barcelona|  4| data scientist|awesome|700000| null|122857.14285714286|254670.65566559267|   2.266232266275318|\n",
      "+---------+---+---------------+-------+------+-----+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = joined_2.withColumn(\"z_score\", (joined_2[\"sueldo\"] - joined_2[\"avg_sueldo\"]) / joined_2[\"std_sueldo\"])\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------------+-------+------+-----+------------------+------------------+--------------------+\n",
      "| location| id|       position|   amen|sueldo|bonus|        avg_sueldo|        std_sueldo|             z_score|\n",
      "+---------+---+---------------+-------+------+-----+------------------+------------------+--------------------+\n",
      "|Barcelona|  4| data scientist|awesome|700000| null|122857.14285714286|254670.65566559267|   2.266232266275318|\n",
      "|   Madrid| 11|project manager|  loser| 40000|10000|           32000.0|10954.451150103323|  0.7302967433402214|\n",
      "|   Madrid| 10|project manager|  loser| 40000| null|           32000.0|10954.451150103323|  0.7302967433402214|\n",
      "|   Madrid|  0|project manager|  loser| 40000| null|           32000.0|10954.451150103323|  0.7302967433402214|\n",
      "|Barcelona|  3|project manager|  loser| 40000| null|122857.14285714286|254670.65566559267|-0.32535017684150597|\n",
      "|Barcelona|  8|project manager|  loser| 40000| null|122857.14285714286|254670.65566559267|-0.32535017684150597|\n",
      "|Barcelona|  6|        teacher|  loser| 20000| null|122857.14285714286|254670.65566559267| -0.4038829781480764|\n",
      "|Barcelona|  5|        teacher|  loser| 20000| null|122857.14285714286|254670.65566559267| -0.4038829781480764|\n",
      "|Barcelona|  2|        teacher|  loser| 20000| null|122857.14285714286|254670.65566559267| -0.4038829781480764|\n",
      "|Barcelona|  7|        teacher|  loser| 20000| null|122857.14285714286|254670.65566559267| -0.4038829781480764|\n",
      "|   Madrid|  9|        teacher|  loser| 20000| 2000|           32000.0|10954.451150103323| -1.0954451150103321|\n",
      "|   Madrid|  1|        teacher|  loser| 20000| null|           32000.0|10954.451150103323| -1.0954451150103321|\n",
      "+---------+---+---------------+-------+------+-----+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.sort(-result[\"z_score\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can build more complex boolean conditions for joining, as well as joining on columns that do not have the same name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---------------+-----+------+-----+\n",
      "| id|location|       position| amen|sueldo|bonus|\n",
      "+---+--------+---------------+-----+------+-----+\n",
      "|  9|  Madrid|        teacher|loser| 20000| 2000|\n",
      "| 11|  Madrid|project manager|loser| 40000|10000|\n",
      "+---+--------+---------------+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined.dropna().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL querying\n",
    "\n",
    "We need to register our DataFrame as a table in the SQL context in order to be able to query against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---------------+-------+------+-----+------------------+------------------+--------------------+\n",
      "| location| id|       position|   amen|sueldo|bonus|        avg_sueldo|        std_sueldo|             z_score|\n",
      "+---------+---+---------------+-------+------+-----+------------------+------------------+--------------------+\n",
      "|   Madrid|  0|project manager|  loser| 40000| null|           32000.0|10954.451150103323|  0.7302967433402214|\n",
      "|   Madrid| 10|project manager|  loser| 40000| null|           32000.0|10954.451150103323|  0.7302967433402214|\n",
      "|   Madrid| 11|project manager|  loser| 40000|10000|           32000.0|10954.451150103323|  0.7302967433402214|\n",
      "|Barcelona|  3|project manager|  loser| 40000| null|122857.14285714286|254670.65566559267|-0.32535017684150597|\n",
      "|Barcelona|  8|project manager|  loser| 40000| null|122857.14285714286|254670.65566559267|-0.32535017684150597|\n",
      "|Barcelona|  4| data scientist|awesome|700000| null|122857.14285714286|254670.65566559267|   2.266232266275318|\n",
      "+---------+---+---------------+-------+------+-----+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.registerTempTable(\"result_table\")\n",
    "\n",
    "session.sql(\"SELECT * FROM result_table WHERE sueldo > 30000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once registered, we can perform queries as complex as we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+----------------------------+\n",
      "| id|       position|SQRT(CAST(sueldo AS DOUBLE))|\n",
      "+---+---------------+----------------------------+\n",
      "|  0|project manager|                       200.0|\n",
      "| 10|project manager|                       200.0|\n",
      "| 11|project manager|                       200.0|\n",
      "|  3|project manager|                       200.0|\n",
      "|  8|project manager|                       200.0|\n",
      "|  4| data scientist|           836.6600265340755|\n",
      "+---+---------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql(\"SELECT id, position, sqrt(sueldo) FROM result_table WHERE sueldo > 30000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Not a function or callable (__call__ is not defined): <class 'pyspark.sql.column.Column'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-2265305b67a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"salary_registered\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msalary_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT id, position, salary_registered(position) FROM result_table WHERE sueldo > 30000\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, name, f, returnType)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturnType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisterFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0mregister\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisterFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36mregisterFunction\u001b[0;34m(self, name, f, returnType)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstringLengthInt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \"\"\"\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisterFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/catalog.py\u001b[0m in \u001b[0;36mregisterFunction\u001b[0;34m(self, name, f, returnType)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstringLengthInt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \"\"\"\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mudf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUserDefinedFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturnType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisterPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/functions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, returnType, name)\u001b[0m\n\u001b[1;32m   1879\u001b[0m             raise TypeError(\n\u001b[1;32m   1880\u001b[0m                 \u001b[0;34m\"Not a function or callable (__call__ is not defined): \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1881\u001b[0;31m                 \"{0}\".format(type(func)))\n\u001b[0m\u001b[1;32m   1882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Not a function or callable (__call__ is not defined): <class 'pyspark.sql.column.Column'>"
     ]
    }
   ],
   "source": [
    "session.udf.register(\"salary_registered\", salary_udf())\n",
    "\n",
    "session.sql(\"SELECT id, position, salary_registered(position) FROM result_table WHERE sueldo > 30000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "replicate the previous exercise, but with SparkSQL instead of dataframe methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interoperation with Pandas\n",
    "\n",
    "Easy peasy. We can convert a spark DataFrame into a Pandas one, which will `collect` it, and viceversa, which will distribute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = joined.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>position</th>\n",
       "      <th>amen</th>\n",
       "      <th>sueldo</th>\n",
       "      <th>bonus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>project manager</td>\n",
       "      <td>loser</td>\n",
       "      <td>40000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>teacher</td>\n",
       "      <td>loser</td>\n",
       "      <td>20000</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>teacher</td>\n",
       "      <td>loser</td>\n",
       "      <td>20000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>project manager</td>\n",
       "      <td>loser</td>\n",
       "      <td>40000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>teacher</td>\n",
       "      <td>loser</td>\n",
       "      <td>20000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>teacher</td>\n",
       "      <td>loser</td>\n",
       "      <td>20000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>project manager</td>\n",
       "      <td>loser</td>\n",
       "      <td>40000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>teacher</td>\n",
       "      <td>loser</td>\n",
       "      <td>20000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>project manager</td>\n",
       "      <td>loser</td>\n",
       "      <td>40000</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>project manager</td>\n",
       "      <td>loser</td>\n",
       "      <td>40000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>awesome</td>\n",
       "      <td>700000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>Madrid</td>\n",
       "      <td>teacher</td>\n",
       "      <td>loser</td>\n",
       "      <td>20000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id   location         position     amen  sueldo    bonus\n",
       "0    3  Barcelona  project manager    loser   40000      NaN\n",
       "1    9     Madrid          teacher    loser   20000   2000.0\n",
       "2    5  Barcelona          teacher    loser   20000      NaN\n",
       "3    0     Madrid  project manager    loser   40000      NaN\n",
       "4    7  Barcelona          teacher    loser   20000      NaN\n",
       "5    6  Barcelona          teacher    loser   20000      NaN\n",
       "6   10     Madrid  project manager    loser   40000      NaN\n",
       "7    2  Barcelona          teacher    loser   20000      NaN\n",
       "8   11     Madrid  project manager    loser   40000  10000.0\n",
       "9    8  Barcelona  project manager    loser   40000      NaN\n",
       "10   4  Barcelona   data scientist  awesome  700000      NaN\n",
       "11   1     Madrid          teacher    loser   20000      NaN"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, location: string, position: string, amen: string, sueldo: bigint, bonus: double]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined.write.csv(\"joined.csv\")\n",
    "# Esto me ha generado una carpeta con diversos archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Repeat the exercise from the previous notebook, but this time with DataFrames.\n",
    "\n",
    "Get stats for all tickets with destination MAD from `coupons150720.csv`.\n",
    "\n",
    "You will need to extract ticket amounts with destination MAD, and then calculate:\n",
    "\n",
    "1. Total ticket amounts per origin\n",
    "2. Top 10 airlines by average amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Extract the fields you need (c0,c1,c2,c3,c4 and c6) into a dataframe with proper names and types\n",
    "\n",
    "Remember, you want to calculate:\n",
    "\n",
    "Total ticket amounts per origin\n",
    "\n",
    "Top 10 airlines by average amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Total ticket amounts per origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Top 10 Airlines by average amount\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/apache-spark-python\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/sql-programming-guide.html\n",
    "\n",
    "https://ogirardot.wordpress.com/2015/05/29/rdds-are-the-new-bytecode-of-apache-spark/\n",
    "\n",
    "https://stackoverflow.com/questions/36822224/what-are-the-pros-and-cons-of-parquet-format-compared-to-other-formats\n",
    "\n",
    "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
